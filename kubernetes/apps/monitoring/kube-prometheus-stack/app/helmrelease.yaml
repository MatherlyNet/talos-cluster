---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
spec:
  chartRef:
    kind: OCIRepository
    name: kube-prometheus-stack
  interval: 1h
  values:
    # Prometheus
    prometheus:
      prometheusSpec:
        retention: "7d"
        # Enable remote write receiver for Tempo metrics generator
        enableRemoteWriteReceiver: true
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: "proxmox-zfs"
              resources:
                requests:
                  storage: "50Gi"
        # Scrape all ServiceMonitors/PodMonitors regardless of labels
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        ruleSelectorNilUsesHelmValues: false
        resources:
          requests:
            cpu: 200m
            memory: 1Gi
          limits:
            memory: 2Gi
        # Talos Linux specific: drop high-cardinality kubelet metrics
        additionalScrapeConfigs: []

    # Grafana
    grafana:
      enabled: true
      grafana.ini:
        feature_toggles:
          enable: exploreMetrics,exploreLogs,traceToLogs,correlations,nestedFolders
        unified_alerting:
          enabled: true
        alerting:
          enabled: false
        users:
          viewers_can_edit: false
        auth.anonymous:
          enabled: false
        analytics:
          reporting_enabled: false
          check_for_updates: false
        log:
          mode: console
          level: warn
        server:
          root_url: "https://grafana.matherly.net"
        auth:
          disable_login_form: true
          disable_signout_menu: false
          oauth_allow_insecure_email_lookup: true
        auth.generic_oauth:
          enabled: true
          name: "Keycloak"
          allow_sign_up: true
          auto_login: true
          client_id: "grafana"
          auth_url: "https://sso.matherly.net/realms/matherlynet/protocol/openid-connect/auth"
          token_url: "https://sso.matherly.net/realms/matherlynet/protocol/openid-connect/token"
          api_url: "https://sso.matherly.net/realms/matherlynet/protocol/openid-connect/userinfo"
          scopes: "openid profile email"
          email_attribute_path: "email"
          login_attribute_path: "preferred_username"
          name_attribute_path: "name"
          role_attribute_path: "contains(roles[*], 'admin') && 'GrafanaAdmin' || contains(roles[*], 'operator') && 'Editor' || contains(roles[*], 'developer') && 'Editor' || 'Viewer'"
          role_attribute_strict: false
          allow_assign_grafana_admin: true
          groups_attribute_path: "groups"
          use_refresh_token: true
          use_pkce: true
      admin:
        existingSecret: grafana-admin-secret
        userKey: admin-user
        passwordKey: admin-password
      envFromSecrets:
        - name: grafana-oidc-secret
          optional: false
      ingress:
        enabled: false
      persistence:
        enabled: true
        storageClassName: "proxmox-zfs"
        size: 5Gi
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          memory: 512Mi
      sidecar:
        dashboards:
          enabled: true
          label: grafana_dashboard
          labelValue: "1"
          searchNamespace: ALL
          folderAnnotation: grafana_folder
          annotations:
            grafana_folder: Monitoring
          provider:
            foldersFromFilesStructure: true
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
            - name: custom
              folder: Custom
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/custom
      dashboards:
        custom:
          # renovate: depName="cert-manager"
          cert-manager:
            gnetId: 20842
            revision: 3
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
          # renovate: depName="Flux Cluster Stats"
          flux-cluster:
            gnetId: 16714
            revision: 1
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
          # renovate: depName="Envoy Gateway Global"
          envoy-gateway:
            gnetId: 24460
            revision: 1
            datasource:
              - name: datasource
                value: Prometheus
      # Additional datasources for unified observability
      additionalDataSources:
        - name: Loki
          uid: loki
          type: loki
          url: http://loki-read.monitoring.svc:3100
          access: proxy
          isDefault: false
          jsonData:
            maxLines: 1000
        - name: Tempo
          uid: tempo
          type: tempo
          url: http://tempo:3200
          access: proxy
          isDefault: false
          jsonData:
            tracesToLogs:
              datasourceUid: loki
              tags: ['namespace', 'pod']
              mappedTags: [{ key: 'service.name', value: 'service' }]
              mapTagNamesEnabled: true
              filterByTraceID: true
              filterBySpanID: true
            tracesToMetrics:
              datasourceUid: prometheus
              tags: [{ key: 'service.name', value: 'service' }]
              queries:
                - name: 'Request Rate'
                  query: 'sum(rate(traces_spanmetrics_calls_total{$$__tags}[5m]))'
            serviceMap:
              datasourceUid: prometheus
            nodeGraph:
              enabled: true
            search:
              hide: false
            lokiSearch:
              datasourceUid: loki

    # AlertManager
    alertmanager:
      enabled: true
      alertmanagerSpec:
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: "proxmox-zfs"
              resources:
                requests:
                  storage: 1Gi
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
          limits:
            memory: 128Mi
      config:
        global:
          resolve_timeout: 5m
        route:
          group_by: ['alertname', 'namespace', 'severity']
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 12h
          receiver: 'null'
          routes:
            - match:
                alertname: Watchdog
              receiver: 'null'
            - match:
                severity: critical
              receiver: 'null'
        receivers:
          - name: 'null'

    # Default alerting rules - enabled with Talos-compatible settings
    defaultRules:
      create: true
      rules:
        alertmanager: true
        etcd: true
        configReloaders: true
        general: true
        k8sContainerCpuUsageSecondsTotal: true
        k8sContainerMemoryCache: true
        k8sContainerMemoryRss: true
        k8sContainerMemorySwap: true
        k8sContainerResource: true
        k8sContainerMemoryWorkingSetBytes: true
        k8sPodOwner: true
        kubeApiserverAvailability: true
        kubeApiserverBurnrate: true
        kubeApiserverHistogram: true
        kubeApiserverSlos: true
        kubeControllerManager: true
        kubelet: true
        kubeProxy: false  # Disabled - Cilium replaces kube-proxy
        kubePrometheusGeneral: true
        kubePrometheusNodeRecording: true
        kubernetesApps: true
        kubernetesResources: true
        kubernetesStorage: true
        kubernetesSystem: true
        kubeSchedulerAlerting: true
        kubeSchedulerRecording: true
        kubeStateMetrics: true
        network: true
        node: true
        nodeExporterAlerting: true
        nodeExporterRecording: true
        prometheus: true
        prometheusOperator: true
        windows: false

    # Node Exporter
    prometheus-node-exporter:
      enabled: true
      resources:
        requests:
          cpu: 20m
          memory: 32Mi
        limits:
          memory: 64Mi

    # kube-state-metrics
    kube-state-metrics:
      enabled: true
      resources:
        requests:
          cpu: 20m
          memory: 64Mi
        limits:
          memory: 128Mi

    # kubelet scraping
    kubelet:
      enabled: true
      serviceMonitor:
        # For Talos Linux - drop high-cardinality metrics
        metricRelabelings:
          - action: labeldrop
            regex: (uid)
          - action: labeldrop
            regex: (id|name)
          - action: drop
            sourceLabels: ["__name__"]
            regex: (rest_client_request_duration_seconds_bucket|rest_client_request_duration_seconds_sum|rest_client_request_duration_seconds_count)

    # etcd monitoring - Talos exposes metrics on HTTP port 2381
    kubeEtcd:
      enabled: true
      endpoints:
        - 192.168.22.101
        - 192.168.22.102
        - 192.168.22.103
      service:
        enabled: true
        port: 2381
        targetPort: 2381

    # API Server
    kubeApiServer:
      enabled: true

    # Controller Manager - Talos Linux binds to 0.0.0.0 but certs only valid for localhost
    kubeControllerManager:
      enabled: true
      endpoints:
        - 192.168.22.101
        - 192.168.22.102
        - 192.168.22.103
      service:
        enabled: true
        port: 10257
        targetPort: 10257
      serviceMonitor:
        https: true
        insecureSkipVerify: true

    # Scheduler - Talos Linux binds to 0.0.0.0 but certs only valid for 127.0.0.1
    kubeScheduler:
      enabled: true
      endpoints:
        - 192.168.22.101
        - 192.168.22.102
        - 192.168.22.103
      service:
        enabled: true
        port: 10259
        targetPort: 10259
      serviceMonitor:
        https: true
        insecureSkipVerify: true

    # kube-proxy disabled - Cilium replaces kube-proxy
    kubeProxy:
      enabled: false

    # Prometheus Operator
    prometheusOperator:
      enabled: true
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          memory: 256Mi
