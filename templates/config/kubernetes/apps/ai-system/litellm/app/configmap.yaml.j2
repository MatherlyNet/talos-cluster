#% if litellm_enabled | default(false) %#
---
#| ============================================================================= #|
#| LITELLM CONFIGURATION - Bootstrap Config                                      #|
#| Models are stored in database (STORE_MODEL_IN_DB=true) for runtime management #|
#| This config provides initial bootstrap settings and general configuration     #|
#| REF: docs/research/litellm-proxy-gateway-integration-jan-2026.md              #|
#| ============================================================================= #|
apiVersion: v1
kind: ConfigMap
metadata:
  name: litellm-config
  namespace: ai-system
data:
  config.yaml: |
    #| Model Definitions (Bootstrap - can be modified via API) #|
    #| REF: https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json #|
    model_list:
#% if azure_openai_us_east_api_key %#
      #| ======================================================================= #|
      #| Azure OpenAI US East Models                                             #|
      #| ======================================================================= #|
      - model_name: gpt-4.1
        litellm_params:
          model: azure/gpt-4.1
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 13107
          rpm: 250
          tpm: 250000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "41"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure/us/gpt-4.1-2025-04-14

      - model_name: gpt-4.1-nano
        litellm_params:
          model: azure/gpt-4.1-nano
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 13107
          rpm: 450
          tpm: 450000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "42"
          access_groups: ["default-models", "aangpt-models", "aanai-models"]
          base_model: azure/us/gpt-4.1-nano-2025-04-14

      - model_name: gpt-4o-mini
        litellm_params:
          model: azure/gpt-4o-mini
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 4096
          rpm: 5000
          tpm: 500000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "40"
          access_groups: ["default-models", "aangpt-models", "aanai-models"]
          base_model: azure/us/gpt-4o-mini-2024-07-18

      - model_name: o3
        litellm_params:
          model: azure/o3
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 40000
          thinking:
            type: enabled
            budget_tokens: 4096
          merge_reasoning_content_in_choices: true
          rpm: 250
          tpm: 250000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "53"
          access_groups: ["premium-models", "restricted-models", "aangpt-models", "aanai-models"]
          base_model: azure/us/o3-2025-04-16

      - model_name: o4-mini
        litellm_params:
          model: azure/o4-mini
          litellm_credential_name: azure_credential_us_east
          max_completion_tokens: 40000
          thinking:
            type: enabled
            budget_tokens: 8192
          merge_reasoning_content_in_choices: true
          rpm: 2000
          tpm: 400000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "54"
          access_groups: ["premium-models", "restricted-models"]
          base_model: azure/us/o4-mini-2025-04-16

      - model_name: text-embedding-3-small
        litellm_params:
          model: azure/text-embedding-3-small
          litellm_credential_name: azure_credential_us_east
          rpm: 3000
          tpm: 500000
        model_info:
          id: "86"
          mode: embedding
          access_groups: ["default-models", "developer-models"]
          base_model: azure/text-embedding-3-small

      - model_name: text-embedding-ada-002
        litellm_params:
          model: azure/text-embedding-ada-002
          litellm_credential_name: azure_credential_us_east
          rpm: 3000
          tpm: 500000
        model_info:
          id: "88"
          mode: embedding
          access_groups: ["default-models", "developer-models"]
          base_model: azure/text-embedding-ada-002
#% endif %#

#% if azure_openai_us_east2_api_key %#
      #| ======================================================================= #|
      #| Azure OpenAI US East2 Models (GPT-5 Series)                             #|
      #| ======================================================================= #|
      - model_name: gpt-5
        litellm_params:
          model: azure/gpt5_series/gpt-5
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 1750
          tpm: 1750000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "10"
          access_groups: ["premium-models", "restricted-models"]
          base_model: azure/us/gpt-5-2025-08-07

      - model_name: gpt-5-chat
        litellm_params:
          model: azure/gpt5_series/gpt-5-chat
          litellm_credential_name: azure_credential_us_east2
          thinking: { "type": "enabled", "budget_tokens": 16384 }
          max_completion_tokens: 16384
          merge_reasoning_content_in_choices: true
          rpm: 1750
          tpm: 1750000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "11"
          mode: chat
          access_groups: ["premium-models", "restricted-models"]
          base_model: azure/gpt-5-chat-latest

      - model_name: gpt-5-mini
        litellm_params:
          model: azure/gpt5_series/gpt-5-mini
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 2500
          tpm: 2500000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "12"
          access_groups: ["default-models"]
          base_model: azure/us/gpt-5-mini-2025-08-07

      - model_name: gpt-5-nano
        litellm_params:
          model: azure/gpt5_series/gpt-5-nano
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 15000
          tpm: 15000000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "13"
          access_groups: ["default-models"]
          base_model: azure/us/gpt-5-nano-2025-08-07

      - model_name: gpt-5.1
        litellm_params:
          model: azure/gpt5_series/gpt-5.1
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 20000
          tpm: 2000000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "6"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models"]
          base_model: azure/global/gpt-5.1

      - model_name: gpt-5.2
        litellm_params:
          model: azure/gpt5_series/gpt-5.2
          litellm_credential_name: azure_credential_us_east2
          max_completion_tokens: 16384
          rpm: 22500
          tpm: 2250000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "1"
          mode: chat
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models"]
          base_model: azure/global/gpt-5.2

      #| Audio Models (disable health checks - LiteLLM doesn't support audio modality checks) #|
      - model_name: gpt-audio
        litellm_params:
          model: azure/gpt-audio
          api_base: "os.environ/AZURE_API_BASE_EAST2"
          api_key: "os.environ/AZURE_API_KEY_EAST2"
          api_version: 2025-01-01-preview
          rpm: 250
          tpm: 250000
        model_info:
          id: "96"
          supports_audio_output: true
          supports_audio_input: true
          access_groups: ["premium-models", "restricted-models"]
          disable_background_health_check: true
          base_model: azure/gpt-audio-2025-08-28

      - model_name: gpt-audio-mini
        litellm_params:
          model: azure/gpt-audio-mini
          api_base: "os.environ/AZURE_API_BASE_EAST2"
          api_key: "os.environ/AZURE_API_KEY_EAST2"
          api_version: 2025-01-01-preview
          rpm: 200
          tpm: 100000
        model_info:
          id: "98"
          supports_audio_output: true
          supports_audio_input: true
          access_groups: ["premium-models", "restricted-models"]
          disable_background_health_check: true
          base_model: azure/gpt-audio-mini-2025-10-06

      #| Image Generation #|
      - model_name: gpt-image-1
        litellm_params:
          model: azure/gpt-image-1
          litellm_credential_name: azure_credential_us_east2
          rpm: 60
        model_info:
          id: "66"
          mode: image_generation
          access_groups: ["premium-models", "restricted-models"]
          base_model: azure/gpt-image-1

      #| Realtime Models (disable health checks - LiteLLM doesn't support realtime checks) #|
      - model_name: gpt-realtime
        litellm_params:
          model: azure/gpt-realtime
          api_base: "os.environ/AZURE_API_BASE_EAST2"
          api_key: "os.environ/AZURE_API_KEY_EAST2"
          api_version: 2024-10-01-preview
          rpm: 200
          tpm: 100000
        model_info:
          id: "92"
          mode: realtime
          access_groups: ["premium-models", "restricted-models"]
          disable_background_health_check: true
          base_model: gpt-realtime-2025-08-28

      - model_name: gpt-realtime-mini
        litellm_params:
          model: azure/gpt-realtime-mini
          api_base: "os.environ/AZURE_API_BASE_EAST2"
          api_key: "os.environ/AZURE_API_KEY_EAST2"
          api_version: 2024-10-01-preview
          rpm: 200
          tpm: 100000
        model_info:
          id: "94"
          mode: realtime
          access_groups: ["premium-models", "restricted-models"]
          disable_background_health_check: true
          base_model: gpt-realtime-mini-2025-10-06

      - model_name: text-embedding-3-large
        litellm_params:
          model: azure/text-embedding-3-large
          litellm_credential_name: azure_credential_us_east2
          rpm: 18000
          tpm: 3000000
        model_info:
          id: "84"
          mode: embedding
          access_groups: ["default-models", "developer-models"]
          base_model: azure/text-embedding-3-large
#% endif %#

#% if azure_anthropic_api_key %#
      #| ======================================================================= #|
      #| Anthropic via Azure (Claude models)                                     #|
      #| ======================================================================= #|
      - model_name: claude-opus-4-5
        litellm_params:
          model: anthropic/claude-opus-4-5
          litellm_credential_name: azure_credential_anthropic_us_east2
          max_completion_tokens: 16384
          rpm: 1500
          tpm: 1500000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "20"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models"]
          base_model: claude-opus-4-5

      - model_name: claude-sonnet-4-5
        litellm_params:
          model: anthropic/claude-sonnet-4-5
          litellm_credential_name: azure_credential_anthropic_us_east2
          max_completion_tokens: 16384
          rpm: 2750
          tpm: 2750000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "21"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models"]
          base_model: claude-sonnet-4-5

      - model_name: claude-opus-4-1
        litellm_params:
          model: anthropic/claude-opus-4-1
          litellm_credential_name: azure_credential_anthropic_us_east2
          max_completion_tokens: 16384
          rpm: 1250
          tpm: 1250000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "22"
          supports_reasoning: true
          access_groups: ["premium-models", "restricted-models"]
          base_model: claude-opus-4-1

      - model_name: claude-haiku-4-5
        litellm_params:
          model: anthropic/claude-haiku-4-5
          litellm_credential_name: azure_credential_anthropic_us_east2
          max_completion_tokens: 16384
          rpm: 2250
          tpm: 2250000
          cache_control_injection_points:
            - location: message
              role: system
        model_info:
          id: "23"
          supports_reasoning: true
          access_groups: ["default-models"]
          base_model: claude-haiku-4-5
#% endif %#

#% if azure_cohere_rerank_api_key %#
      #| Cohere Rerank #|
      - model_name: cohere-rerank-v3.5
        litellm_params:
          model: cohere/cohere-rerank-v3.5
          api_key: "os.environ/AZURE_COHERE_RERANK_API_KEY"
          api_base: "os.environ/AZURE_COHERE_RERANK_API_BASE"
          max_output_tokens: 4096
        model_info:
          id: "76"
          mode: rerank
          access_groups: ["premium-models", "restricted-models"]
          base_model: cohere-rerank-v3.5
#% endif %#

#% if azure_cohere_embed_api_key %#
      #| Cohere Embed (disable health check - Azure AI returns 404) #|
      - model_name: cohere-embed-v-4-0
        litellm_params:
          model: azure_ai/cohere-embed-v-4-0
          api_key: "os.environ/AZURE_COHERE_EMBED_API_KEY"
          api_base: "os.environ/AZURE_COHERE_EMBED_API_BASE"
          input_type: text
        model_info:
          id: "77"
          mode: embedding
          access_groups: ["premium-models", "restricted-models"]
          base_model: cohere-embed-v-4-0
          disable_background_health_check: true
#% endif %#

    #| ======================================================================= #|
    #| Credential List                                                         #|
    #| ======================================================================= #|
    credential_list:
#% if azure_openai_us_east_api_key %#
      - credential_name: azure_credential_us_east
        credential_values:
          api_key: "os.environ/AZURE_API_KEY"
          api_base: "os.environ/AZURE_API_BASE"
          api_version: "os.environ/AZURE_API_VERSION"
        credential_info:
          description: "Azure OpenAI US East credentials"
#% endif %#
#% if azure_openai_us_east2_api_key %#
      - credential_name: azure_credential_us_east2
        credential_values:
          api_key: "os.environ/AZURE_API_KEY_EAST2"
          api_base: "os.environ/AZURE_API_BASE_EAST2"
          api_version: "os.environ/AZURE_API_VERSION_EAST2"
        credential_info:
          description: "Azure OpenAI US East2 credentials"
#% endif %#
#% if azure_anthropic_api_key %#
      - credential_name: azure_credential_anthropic_us_east2
        credential_values:
          api_key: "os.environ/AZURE_ANTHROPIC_API_KEY"
          api_base: "os.environ/AZURE_ANTHROPIC_API_BASE"
        credential_info:
          description: "Azure Anthropic US East2 credentials"
#% endif %#

    #| ======================================================================= #|
    #| LiteLLM Settings                                                        #|
    #| ======================================================================= #|
    litellm_settings:
      drop_params: true

      #| Callbacks for metrics and observability #|
#% if litellm_langfuse_enabled | default(false) %#
      success_callback:
        - "prometheus"
        - "langfuse"
      failure_callback:
        - "prometheus"
        - "langfuse"
#% else %#
      success_callback:
        - "prometheus"
      failure_callback:
        - "prometheus"
#% endif %#
      callbacks:
        - "prometheus"
      service_callbacks:
        - "prometheus_system"

#% if litellm_alerting_enabled | default(false) %#
      #| ======================================================================= #|
      #| Alerting Configuration (Slack/Discord)                                  #|
      #| REF: https://docs.litellm.ai/docs/proxy/alerting                        #|
      #| ======================================================================= #|
      alerting:
#% if litellm_slack_webhook_url %#
        - slack
#% endif %#
#% if litellm_discord_webhook_url %#
        - discord
#% endif %#
      alerting_threshold: #{ litellm_alerting_threshold | default(300) }#
      alerting_args:
#% if litellm_slack_webhook_url %#
        slack_webhook_url: "os.environ/SLACK_WEBHOOK_URL"
#% endif %#
#% if litellm_discord_webhook_url %#
        discord_webhook_url: "os.environ/DISCORD_WEBHOOK_URL"
#% endif %#
#% endif %#

#% if litellm_guardrails_enabled | default(false) or litellm_presidio_enabled | default(false) or litellm_prompt_injection_check | default(false) %#
      #| ======================================================================= #|
      #| Guardrails Configuration (Content Safety)                               #|
      #| REF: https://docs.litellm.ai/docs/proxy/guardrails/                     #|
      #| ======================================================================= #|
      guardrails:
#% if litellm_guardrails_enabled | default(false) %#
        - guardrail_name: "content-filter"
          litellm_params:
            guardrail: "aporia"
            mode: "during_call"
            api_base: "os.environ/APORIA_API_BASE"
            api_key: "os.environ/APORIA_API_KEY"
#% endif %#
#% if litellm_presidio_enabled | default(false) %#
        - guardrail_name: "pii-masking"
          litellm_params:
            guardrail: "presidio"
            mode: "pre_call"
            presidio_analyzer_api_base: "http://localhost:5001"
            presidio_anonymizer_api_base: "http://localhost:5002"
            output_parse_pii: true
#% endif %#
#% if litellm_prompt_injection_check | default(false) %#
        - guardrail_name: "prompt-injection"
          litellm_params:
            guardrail: "lakera_prompt_injection"
            mode: "pre_call"
            api_base: "os.environ/LAKERA_API_BASE"
            api_key: "os.environ/LAKERA_API_KEY"
#% endif %#
#% endif %#

      #| Privacy settings #|
      turn_off_message_logging: true
      redact_user_api_key_info: true
      redact_messages_in_exceptions: true
      set_verbose: false
      json_logs: true

      #| Networking & Timeout Settings #|
      request_timeout: 30

#% if litellm_langfuse_enabled | default(false) %#
      #| Langfuse Settings #|
      langfuse_host: "os.environ/LANGFUSE_HOST"
      langfuse_public_key: "os.environ/LANGFUSE_PUBLIC_KEY"
      langfuse_secret: "os.environ/LANGFUSE_SECRET_KEY"
      langfuse_enabled: true
      langfuse_sample_rate: 1.0
#% endif %#

      #| Caching Settings (Dragonfly/Redis) #|
      cache: true
      cache_params:
        type: redis
        host: "os.environ/REDIS_HOST"
        port: "os.environ/REDIS_PORT"
        password: "os.environ/REDIS_PASSWORD"
        namespace: "os.environ/REDIS_NAMESPACE"
        max_connections: 100
        supported_call_types: ["acompletion", "atext_completion", "aembedding", "atranscription"]
        mode: default_on
        ttl: 600

      #| Rate Limiting #|
      telemetry: false
      tpm_limit: 3500000
      rpm_limit: 35000
      max_file_size_mb: 25
      max_budget: 1000
      budget_duration: 30d

      #| Cost Tracking #|
      track_cost_per_model: true
      track_cost_per_team: true
      track_cost_per_user: true

    #| ======================================================================= #|
    #| General Settings                                                        #|
    #| ======================================================================= #|
    general_settings:
      master_key: "os.environ/LITELLM_MASTER_KEY"
      database_url: "os.environ/DATABASE_URL"
      database_connection_pool_limit: 20
      database_connection_timeout: 60
      allow_requests_on_db_unavailable: true
      disable_spend_logs: false
      store_model_in_db: "os.environ/STORE_MODEL_IN_DB"
      store_prompts_in_spend_logs: true
      proxy_batch_write_at: 60
      proxy_budget_rescheduler_min_time: 300
      proxy_budget_rescheduler_max_time: 3600
      disable_error_logs: true

      #| Health checks disabled due to Prisma timeout issues #|
      #| REF: https://github.com/BerriAI/litellm/issues/7196 #|
      background_health_checks: false
      health_check_interval: 300

    #| ======================================================================= #|
    #| Router Settings                                                         #|
    #| ======================================================================= #|
    router_settings:
      routing_strategy: simple-shuffle
      enable_pre_call_checks: true
      enable_tag_filtering: true
      timeout: 120
      stream_timeout: 300

      #| Multi-Instance Coordination (Dragonfly/Redis) #|
      redis_host: "os.environ/REDIS_HOST"
      redis_port: "os.environ/REDIS_PORT"
      redis_password: "os.environ/REDIS_PASSWORD"

      #| Failover & Retry Policies #|
      allowed_fails: 3
      cooldown_time: 30
      disable_cooldowns: true
      retry_policy:
        AuthenticationErrorRetries: 1
        TimeoutErrorRetries: 2
        RateLimitErrorRetries: 3
        ContentPolicyViolationErrorRetries: 2
        InternalServerErrorRetries: 3
      allowed_fails_policy:
        BadRequestErrorAllowedFails: 1000
        AuthenticationErrorAllowedFails: 10
        TimeoutErrorAllowedFails: 12
        RateLimitErrorAllowedFails: 10000
        ContentPolicyViolationErrorAllowedFails: 15
        InternalServerErrorAllowedFails: 20

      debug_level: "INFO"
#% endif %#
