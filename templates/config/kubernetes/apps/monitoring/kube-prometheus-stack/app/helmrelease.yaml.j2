#% if monitoring_enabled | default(false) and monitoring_stack == 'prometheus' %#
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
spec:
  chartRef:
    kind: OCIRepository
    name: kube-prometheus-stack
  interval: 1h
  values:
    # Prometheus
    prometheus:
      prometheusSpec:
        retention: "#{ metrics_retention | default('7d') }#"
        # Enable remote write receiver for Tempo metrics generator
        enableRemoteWriteReceiver: true
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: "#{ storage_class | default('local-path') }#"
              resources:
                requests:
                  storage: "#{ metrics_storage_size | default('50Gi') }#"
        # Scrape all ServiceMonitors/PodMonitors regardless of labels
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        ruleSelectorNilUsesHelmValues: false
        resources:
          requests:
            cpu: 200m
            memory: 1Gi
          limits:
            memory: 2Gi
        # Talos Linux specific: drop high-cardinality kubelet metrics
        additionalScrapeConfigs: []

    # Grafana
    grafana:
      enabled: true
      #| Grafana.ini configuration for Grafana 12+ features #|
      grafana.ini:
        feature_toggles:
          #| Enable Grafana 12 Drilldown apps (formerly Explore Metrics/Logs/Traces) #|
          enable: exploreMetrics,exploreLogs,traceToLogs,correlations,nestedFolders
        unified_alerting:
          #| Grafana-managed alerting (replaces legacy alerting removed in Grafana 11) #|
          enabled: true
        alerting:
          #| Disable legacy alerting (deprecated) #|
          enabled: false
        users:
          #| Allow viewing dashboards without login (read-only) #|
          viewers_can_edit: false
        auth.anonymous:
          #| Anonymous access disabled by default - enable if you want public dashboards #|
          enabled: false
        analytics:
          #| Disable analytics reporting #|
          reporting_enabled: false
          check_for_updates: false
        log:
          mode: console
          level: warn
      #| Admin credentials from SOPS-encrypted secret #|
      admin:
        existingSecret: grafana-admin-secret
        userKey: admin-user
        passwordKey: admin-password
      #| Ingress disabled - using Gateway API HTTPRoute for envoy-internal instead #|
      ingress:
        enabled: false
      persistence:
        enabled: true
        storageClassName: "#{ storage_class | default('local-path') }#"
        size: 5Gi
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          memory: 512Mi
      #| Sidecar for auto-discovery of dashboard ConfigMaps #|
      sidecar:
        dashboards:
          enabled: true
          label: grafana_dashboard
          labelValue: "1"
          searchNamespace: ALL
          #| Enable folder organization via grafana_folder annotation #|
          folderAnnotation: grafana_folder
          #| Add grafana_folder annotation to ALL built-in dashboard ConfigMaps #|
          #| This organizes AlertManager, CoreDNS, etcd, k8s resources, kubelet, #|
          #| node exporter, Prometheus, scheduler, etc. into a single folder #|
          annotations:
            grafana_folder: Monitoring
          provider:
            foldersFromFilesStructure: true
      #| Dashboard provisioning - custom dashboards not included in kube-prometheus-stack #|
      #| Built-in dashboards: CoreDNS, etcd, k8s resources, nodes, controller-manager, scheduler, etc. #|
      dashboardProviders:
        dashboardproviders.yaml:
          apiVersion: 1
          providers:
            - name: custom
              folder: Custom
              type: file
              disableDeletion: false
              editable: true
              options:
                path: /var/lib/grafana/dashboards/custom
      #| Only provision dashboards NOT included in kube-prometheus-stack built-ins #|
      dashboards:
        custom:
          #| cert-manager - certificate monitoring (not included in kube-prometheus-stack) #|
          cert-manager:
            gnetId: 20842
            revision: 3
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
          #| Cilium Agent - BPF operations, API latency, forwarding stats #|
          cilium-agent:
            gnetId: 16611
            revision: 1
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
          #| Cilium Operator - IPAM, node management #|
          cilium-operator:
            gnetId: 16612
            revision: 1
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
          #| Cilium Hubble - flows, drops, DNS, HTTP, TCP #|
          cilium-hubble:
            gnetId: 16613
            revision: 1
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
          #| Flux CD - GitOps reconciliation monitoring #|
          flux-cluster:
            gnetId: 16714
            revision: 1
            datasource:
              - name: DS_PROMETHEUS
                value: Prometheus
          #| Envoy Gateway - gateway metrics (not included in kube-prometheus-stack) #|
          envoy-gateway:
            gnetId: 24460
            revision: 1
            datasource:
              - name: datasource
                value: Prometheus
      # Additional datasources for unified observability
      additionalDataSources:
#% if loki_enabled | default(false) %#
        - name: Loki
          uid: loki
          type: loki
          url: http://loki:3100
          access: proxy
          isDefault: false
          jsonData:
            maxLines: 1000
#% endif %#
#% if tracing_enabled | default(false) %#
        - name: Tempo
          uid: tempo
          type: tempo
          url: http://tempo:3200
          access: proxy
          isDefault: false
          jsonData:
            tracesToLogs:
              datasourceUid: loki
              tags: ['namespace', 'pod']
              mappedTags: [{ key: 'service.name', value: 'service' }]
              mapTagNamesEnabled: true
              filterByTraceID: true
              filterBySpanID: true
            tracesToMetrics:
              datasourceUid: prometheus
              tags: [{ key: 'service.name', value: 'service' }]
              queries:
                - name: 'Request Rate'
                  query: 'sum(rate(traces_spanmetrics_calls_total{$$__tags}[5m]))'
            serviceMap:
              datasourceUid: prometheus
            nodeGraph:
              enabled: true
            search:
              hide: false
            lokiSearch:
              datasourceUid: loki
#% endif %#

    # AlertManager
    alertmanager:
      enabled: true
      alertmanagerSpec:
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: "#{ storage_class | default('local-path') }#"
              resources:
                requests:
                  storage: 1Gi
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
          limits:
            memory: 128Mi
      config:
        global:
          resolve_timeout: 5m
        route:
          group_by: ['alertname', 'namespace', 'severity']
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 12h
          receiver: 'null'
          routes:
            - match:
                alertname: Watchdog
              receiver: 'null'
            - match:
                severity: critical
              receiver: 'null'
        receivers:
          - name: 'null'

    # Default alerting rules - enabled with Talos-compatible settings
    defaultRules:
      create: true
      rules:
        alertmanager: true
        etcd: true
        configReloaders: true
        general: true
        k8sContainerCpuUsageSecondsTotal: true
        k8sContainerMemoryCache: true
        k8sContainerMemoryRss: true
        k8sContainerMemorySwap: true
        k8sContainerResource: true
        k8sContainerMemoryWorkingSetBytes: true
        k8sPodOwner: true
        kubeApiserverAvailability: true
        kubeApiserverBurnrate: true
        kubeApiserverHistogram: true
        kubeApiserverSlos: true
        kubeControllerManager: true
        kubelet: true
        kubeProxy: false  # Disabled - Cilium replaces kube-proxy
        kubePrometheusGeneral: true
        kubePrometheusNodeRecording: true
        kubernetesApps: true
        kubernetesResources: true
        kubernetesStorage: true
        kubernetesSystem: true
        kubeSchedulerAlerting: true
        kubeSchedulerRecording: true
        kubeStateMetrics: true
        network: true
        node: true
        nodeExporterAlerting: true
        nodeExporterRecording: true
        prometheus: true
        prometheusOperator: true
        windows: false

    # Node Exporter
    prometheus-node-exporter:
      enabled: true
      resources:
        requests:
          cpu: 20m
          memory: 32Mi
        limits:
          memory: 64Mi

    # kube-state-metrics
    kube-state-metrics:
      enabled: true
      resources:
        requests:
          cpu: 20m
          memory: 64Mi
        limits:
          memory: 128Mi

    # kubelet scraping
    kubelet:
      enabled: true
      serviceMonitor:
        # For Talos Linux - drop high-cardinality metrics
        metricRelabelings:
          - action: labeldrop
            regex: (uid)
          - action: labeldrop
            regex: (id|name)
          - action: drop
            sourceLabels: ["__name__"]
            regex: (rest_client_request_duration_seconds_bucket|rest_client_request_duration_seconds_sum|rest_client_request_duration_seconds_count)

    # etcd monitoring - Talos exposes metrics on HTTP port 2381
    kubeEtcd:
      enabled: true
      endpoints:
#% for node in nodes %#
#% if node.controller %#
        - #{ node.address }#
#% endif %#
#% endfor %#
      service:
        enabled: true
        port: 2381
        targetPort: 2381

    # API Server
    kubeApiServer:
      enabled: true

    # Controller Manager - Talos Linux binds to 0.0.0.0 but certs only valid for localhost
    kubeControllerManager:
      enabled: true
      endpoints:
#% for node in nodes %#
#% if node.controller %#
        - #{ node.address }#
#% endif %#
#% endfor %#
      service:
        enabled: true
        port: 10257
        targetPort: 10257
      serviceMonitor:
        https: true
        insecureSkipVerify: true

    # Scheduler - Talos Linux binds to 0.0.0.0 but certs only valid for 127.0.0.1
    kubeScheduler:
      enabled: true
      endpoints:
#% for node in nodes %#
#% if node.controller %#
        - #{ node.address }#
#% endif %#
#% endfor %#
      service:
        enabled: true
        port: 10259
        targetPort: 10259
      serviceMonitor:
        https: true
        insecureSkipVerify: true

    # kube-proxy disabled - Cilium replaces kube-proxy
    kubeProxy:
      enabled: false

    # Prometheus Operator
    prometheusOperator:
      enabled: true
      resources:
        requests:
          cpu: 50m
          memory: 64Mi
        limits:
          memory: 256Mi
#% endif %#
