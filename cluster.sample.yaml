# yaml-language-server: $schema=./.taskfiles/template/resources/cluster.schema.json
---
# =============================================================================
# TALOS CLUSTER - Required
# =============================================================================
# -- The network CIDR for the nodes.
# (REQUIRED) / (e.g. 192.168.1.0/24)
node_cidr: ""

# -- DNS servers to use for the cluster.
#    (OPTIONAL) / (DEFAULT: ["1.1.1.1", "1.0.0.1"]) / (Cloudflare DNS)
# node_dns_servers: []

# -- NTP servers to use for the cluster.
#    (OPTIONAL) / (DEFAULT: ["162.159.200.1", "162.159.200.123"]) / (Cloudflare NTP)
# node_ntp_servers: []

# -- The default gateway for the nodes.
#    (OPTIONAL) / (DEFAULT: the first IP in the node_cidr)
# node_default_gateway: ""

# -- Attach a vlan tag to the Talos nodes. Not needed if ports on your switch are tagged or you are not using VLANs.
#    (OPTIONAL) / (REF: https://www.talos.dev/latest/advanced/advanced-networking/#vlans)
# node_vlan_tag: ""

# -- When true, Proxmox handles VLAN tagging (access port mode) and Talos sees untagged traffic.
#    When false (default), Talos creates VLAN sub-interfaces (trunk port / bare-metal mode).
#    Set to true when using OpenTofu/Proxmox VM provisioning with node_vlan_tag.
#    (OPTIONAL) / (DEFAULT: false)
# proxmox_vlan_mode: false

# -- The IP address of the Kube API.
#    (REQUIRED) / (NOTE: Choose an unused IP in node_cidr)
cluster_api_addr: ""

# -- Additional SANs to add to the Kube API cert. This is useful if you want to call the Kube API by hostname rather than IP
#    (OPTIONAL) / (e.g. ["mycluster.example.com"])
# cluster_api_tls_sans: []

# -- The pod CIDR for the cluster, this must NOT overlap with any existing networks and should be a /16 (64K IPs).
#    (REQUIRED) / (DEFAULT: "10.42.0.0/16")
cluster_pod_cidr: ""

# -- The service CIDR for the cluster, this must NOT overlap with any existing networks and should be a /16 (64K IPs).
#    (REQUIRED) / (DEFAULT: "10.43.0.0/16")
cluster_svc_cidr: ""

# -- The Load balancer IP for k8s_gateway (split DNS for internal resolution)
#    (CONDITIONAL) / Required ONLY when NOT using UniFi DNS integration
#    When unifi_host and unifi_api_key are set, k8s-gateway is replaced by
#    external-dns-unifi and this setting is ignored.
#    (NOTE: Choose an unused IP in node_cidr if using k8s_gateway)
# cluster_dns_gateway_addr: ""

# -- The Load balancer IP for the internal gateway
#    (REQUIRED) / (NOTE: Choose an unused IP in node_cidr)
cluster_gateway_addr: ""

# -- GitHub repository
#    (REQUIRED) / (e.g. "onedr0p/cluster-template")
repository_name: ""

# -- GitHub repository branch
#    (OPTIONAL) / (DEFAULT: "main")
# repository_branch: ""

# -- Repository visibility (public or private)
#    (OPTIONAL) / (DEFAULT: "public") / (NOTE: See the README for information when set private)
# repository_visibility: ""

# -- Domain you wish to use from your Cloudflare account
#    (REQUIRED) / (e.g. "example.com")
cloudflare_domain: ""

# -- API Token for Cloudflare with the 'Zone:DNS:Edit' and 'Account:Cloudflare Tunnel:Read' permissions
#    (REQUIRED) (NOTE: See the README for information on creating this)
cloudflare_token: ""

# -- The Load balancer IP for the external gateway
#    (REQUIRED) / (NOTE: Choose an unused IP in node_cidr)
cloudflare_gateway_addr: ""
# =============================================================================
# CILIUM BGP CONFIGURATION - Optional for multi-VLAN environments
# =============================================================================
# Enable BGP peering between Cilium and your router for dynamic routing.
# This allows LoadBalancer IPs to be advertised via BGP instead of L2/ARP.
#
# When to use BGP:
# - Multi-VLAN environment requiring cross-subnet service access
# - Faster failover needed (~9s with tuned timers vs ARP cache timeout)
# - Source IP preservation with externalTrafficPolicy: Local
#
# Requirements:
# - UniFi gateway with UniFi OS 4.1.13+ (or UXG-Enterprise 4.1.8+)
# - FRR config uploaded to gateway (generated in unifi/bgp.conf after configure)
#
# REF: https://docs.cilium.io/en/stable/network/bgp-control-plane/bgp-control-plane-v2/
# REF: docs/guides/bgp-unifi-cilium-implementation.md

# -- The IP address of the BGP router (your gateway on the node network)
#    (OPTIONAL) / (e.g. "192.168.1.1" or VLAN gateway like "192.168.23.254")
#    NOTE: Use the gateway IP on the same VLAN as your nodes, not the management IP
# cilium_bgp_router_addr: ""

# -- The BGP ASN for your router (use private range 64512-65534)
#    (OPTIONAL) / (e.g. "64513")
# cilium_bgp_router_asn: ""

# -- The BGP ASN for Kubernetes nodes (must differ from router ASN for eBGP)
#    (OPTIONAL) / (e.g. "64514")
# cilium_bgp_node_asn: ""

# -- Dedicated CIDR for LoadBalancer IPs (separate from node_cidr)
#    (OPTIONAL) / (e.g. "172.20.10.0/24")
#    NOTE: If set, update UniFi prefix-list to match this CIDR
# cilium_lb_pool_cidr: ""

# -- The load balancer mode for cilium.
#    (OPTIONAL) / (DEFAULT: "dsr") / (NOTE: accepted values are 'dsr' or 'snat') / (REF: https://docs.cilium.io/en/stable/network/kubernetes/kubeproxy-free/)
# cilium_loadbalancer_mode: ""

# -- BGP Hold Time in seconds (failure detection = 3x keepalive or hold timeout)
#    (OPTIONAL) / (DEFAULT: 30) / (Minimum: 3)
#    NOTE: Lower values = faster failure detection, but more BGP traffic
# cilium_bgp_hold_time: 30

# -- BGP Keepalive Time in seconds (sent every N seconds to peer)
#    (OPTIONAL) / (DEFAULT: 10) / (Minimum: 1)
# cilium_bgp_keepalive_time: 10

# -- Enable BGP Graceful Restart for smoother failover during Cilium restarts
#    (OPTIONAL) / (DEFAULT: false)
#    NOTE: Requires UniFi FRR config to also enable graceful-restart
# cilium_bgp_graceful_restart: false

# -- Graceful Restart timeout in seconds
#    (OPTIONAL) / (DEFAULT: 120)
# cilium_bgp_graceful_restart_time: 120

# -- Maximum ECMP paths for load balancing across advertising nodes
#    (OPTIONAL) / (DEFAULT: 3)
#    NOTE: When multiple nodes advertise the same LoadBalancer IP, traffic is
#    distributed across up to this many paths for better load distribution
# cilium_bgp_ecmp_max_paths: 3

# -- MD5 password for BGP session authentication (RFC 2385)
#    (OPTIONAL) / Must match password configured in UniFi FRR config
#    NOTE: When set, a Kubernetes Secret will be created and referenced by
#    CiliumBGPPeerConfig for TCP MD5 authentication
# cilium_bgp_password: ""

# =============================================================================
# CILIUM NETWORK POLICIES - Zero-Trust Network Security
# =============================================================================
# CiliumNetworkPolicies enforce zero-trust networking with L3-L7 controls.
# Policies are applied per-namespace with explicit ingress/egress allowlists.
# REF: https://docs.cilium.io/en/stable/security/policy/
# REF: docs/research/cilium-network-policies-jan-2026.md

# -- Enable CiliumNetworkPolicies for zero-trust networking
#    Creates namespace-scoped policies with default-deny and explicit allowlists
#    (OPTIONAL) / (DEFAULT: false)
# network_policies_enabled: false

# -- Network policy enforcement mode: "audit" or "enforce"
#    audit: Policies created with enableDefaultDeny: false (observe only via Hubble)
#    enforce: Policies created with enableDefaultDeny: true (active blocking)
#    Start with audit mode to observe traffic patterns before enforcing
#    (OPTIONAL) / (DEFAULT: "audit")
# network_policies_mode: "audit"

# =============================================================================
# UNIFI DNS INTEGRATION - Optional for internal DNS via external-dns webhook
# =============================================================================
# When configured, replaces k8s_gateway with native UniFi DNS record management.
# Requires UniFi Network v9.0.0+ for API key authentication (current stable: 9.5.21)

# -- The UniFi controller host URL
#    (OPTIONAL) / (e.g. "https://192.168.1.1")
# unifi_host: ""

# -- The UniFi API key for DNS management
#    (OPTIONAL) / (Created in UniFi Admin → Control Plane → Integrations)
# unifi_api_key: ""

# -- The UniFi site identifier
#    (OPTIONAL) / (DEFAULT: "default")
# unifi_site: ""

# -- Whether using non-UDM hardware (Cloud Key, self-hosted controller)
#    (OPTIONAL) / (DEFAULT: false)
# unifi_external_controller: false

# =============================================================================
# TALOS UPGRADE CONTROLLER (tuppr) - Automated OS/K8s upgrades
# =============================================================================
# tuppr manages Talos OS and Kubernetes upgrades through GitOps-driven CRs.
# Update these versions to trigger automated rolling upgrades.

# -- Talos OS version for automated upgrades
#    (OPTIONAL) / (DEFAULT: "1.12.0")
# talos_version: "1.12.0"

# -- Kubernetes version for automated upgrades
#    (OPTIONAL) / (DEFAULT: "1.35.0")
# kubernetes_version: "1.35.0"

# =============================================================================
# PROXMOX CSI CONFIGURATION - Optional for persistent storage
# =============================================================================
# Proxmox CSI provisions PersistentVolumes directly on Proxmox storage.
# Requires Proxmox API token with storage permissions.

# -- Enable Proxmox CSI for persistent storage
#    (OPTIONAL) / (DEFAULT: false)
# proxmox_csi_enabled: false

# -- Proxmox API endpoint (shared with other Proxmox integrations)
#    (OPTIONAL) / (e.g. "https://pve.example.com:8006")
# proxmox_endpoint: ""

# -- Proxmox CSI API token ID (format: user@realm!token-name)
#    (OPTIONAL) / (e.g. "kubernetes-csi@pve!csi")
# proxmox_csi_token_id: ""

# -- Proxmox CSI API token secret
#    (OPTIONAL) / (Will be SOPS-encrypted after task configure)
# proxmox_csi_token_secret: ""

# -- Proxmox storage pool for PVs
#    (OPTIONAL) / (e.g. "local-zfs")
# proxmox_csi_storage: ""

# -- Proxmox region identifier (cluster name)
#    (OPTIONAL) / (DEFAULT: "pve")
# proxmox_region: "pve"

# =============================================================================
# PROXMOX CCM CONFIGURATION - Optional for node labeling/lifecycle
# =============================================================================
# Proxmox CCM provides node labeling, lifecycle management, and topology awareness.
# Use Proxmox CCM instead of Talos CCM when running on Proxmox infrastructure.
# NOTE: Talos CCM and Proxmox CCM are mutually exclusive - only one can be enabled.

# -- Enable Proxmox CCM (disables Talos CCM)
#    (OPTIONAL) / (DEFAULT: false)
# proxmox_ccm_enabled: false

# -- Proxmox CCM API token ID (format: user@realm!token-name)
#    (OPTIONAL) / (e.g. "kubernetes-ccm@pve!ccm")
#    NOTE: Use separate token from CSI for least-privilege principle
# proxmox_ccm_token_id: ""

# -- Proxmox CCM API token secret
#    (OPTIONAL) / (Will be SOPS-encrypted after task configure)
# proxmox_ccm_token_secret: ""

# =============================================================================
# INFRASTRUCTURE (OpenTofu/Proxmox) - Optional for VM deployments
# =============================================================================

# -- Proxmox API endpoint
#    (OPTIONAL) / (e.g. "https://pve.example.com:8006/api2/json")
#    Required only for automated VM provisioning via OpenTofu
# proxmox_api_url: ""

# -- Proxmox node name to create VMs on
#    (OPTIONAL) / (e.g. "pve")
# proxmox_node: ""

# -- Proxmox storage for ISO images
#    (OPTIONAL) / (DEFAULT: "local")
# proxmox_iso_storage: ""

# -- Proxmox storage for VM disks
#    (OPTIONAL) / (DEFAULT: "local-lvm")
# proxmox_disk_storage: ""

# -- Proxmox API token for OpenTofu VM provisioning
#    Create at: Datacenter → Permissions → API Tokens → Add
#    Format: user@realm!token-name (e.g., root@pam!terraform)
#    NOTE: Separate from CSI/CCM tokens for least-privilege
#    (OPTIONAL) / Required when infrastructure_enabled
# proxmox_api_token_id: ""
# proxmox_api_token_secret: ""

# =============================================================================
# INFRASTRUCTURE CREDENTIALS (OpenTofu R2 State Backend)
# =============================================================================
# Credentials for the tfstate-worker HTTP backend on Cloudflare R2.
# These are used by `task infra:*` commands for state management.

# -- Cloudflare Account ID
#    Dashboard → Overview → Account ID (right sidebar)
#    (OPTIONAL) / Required for R2 state backend
# cf_account_id: ""

# -- tfstate-worker Basic Auth credentials
#    Must match secrets configured in your tfstate-worker deployment
#    (OPTIONAL) / (DEFAULT: "terraform")
# tfstate_username: "terraform"
# tfstate_password: ""

# =============================================================================
# OBSERVABILITY - Monitoring Stack (kube-prometheus-stack)
# =============================================================================
# Full-stack observability with metrics, logs, and distributed tracing.
# Uses kube-prometheus-stack: Prometheus + Grafana + AlertManager + node-exporter

# -- Enable monitoring stack (Prometheus + Grafana + AlertManager)
#    (OPTIONAL) / (DEFAULT: false)
# monitoring_enabled: false

# -- Monitoring stack choice
#    Currently only "prometheus" (kube-prometheus-stack) is supported
#    (OPTIONAL) / (DEFAULT: "prometheus")
# monitoring_stack: "prometheus"

# -- Enable Hubble network observability (requires monitoring_enabled)
#    Provides network flow visibility via Cilium
#    (OPTIONAL) / (DEFAULT: false)
# hubble_enabled: false

# -- Enable Hubble UI web interface
#    (OPTIONAL) / (DEFAULT: false)
# hubble_ui_enabled: false

# -- Grafana subdomain (creates grafana.<cloudflare_domain>)
#    (OPTIONAL) / (DEFAULT: "grafana")
# grafana_subdomain: "grafana"

# -- Grafana admin username
#    (OPTIONAL) / (DEFAULT: "admin")
# grafana_admin_user: "admin"

# -- Grafana admin password (SOPS-encrypted)
#    (OPTIONAL) / Set BEFORE initial deployment - password changes after first
#    login require CLI reset: kubectl -n monitoring exec deploy/kube-prometheus-stack-grafana
#    -- grafana-cli admin reset-admin-password <new-password>
# grafana_admin_password: ""

# -- Metrics retention period
#    (OPTIONAL) / (DEFAULT: "7d")
# metrics_retention: "7d"

# -- Metrics storage size
#    (OPTIONAL) / (DEFAULT: "50Gi")
# metrics_storage_size: "50Gi"

# -- Storage class for monitoring (uses proxmox-zfs if available)
#    (OPTIONAL) / (DEFAULT: "local-path")
# storage_class: "local-path"

# -- Enable infrastructure alerting rules (PrometheusRule)
#    Generates alerts for: Node health, Control Plane, etcd, Cilium, CoreDNS,
#    Envoy Gateway, Certificates, Flux GitOps, Workloads, Storage
#    (OPTIONAL) / (DEFAULT: true)
# monitoring_alerts_enabled: true

# -- Memory utilization % threshold for NodeMemoryHighUtilization alert
#    (OPTIONAL) / (DEFAULT: 90) / (Range: 50-99)
# node_memory_threshold: 90

# -- CPU utilization % threshold for NodeCPUHighUtilization alert
#    (OPTIONAL) / (DEFAULT: 90) / (Range: 50-99)
# node_cpu_threshold: 90

# -- Enable log aggregation with Loki
#    (OPTIONAL) / (DEFAULT: false)
# loki_enabled: false

# -- Log retention period
#    (OPTIONAL) / (DEFAULT: "7d")
# logs_retention: "7d"

# -- Log storage size
#    (OPTIONAL) / (DEFAULT: "50Gi")
# logs_storage_size: "50Gi"

# =============================================================================
# OBSERVABILITY - Distributed Tracing (Optional)
# =============================================================================
# Tempo for distributed tracing with Alloy as the unified collector.

# -- Enable distributed tracing with Tempo
#    Requires monitoring_enabled: true
#    (OPTIONAL) / (DEFAULT: false)
# tracing_enabled: false

# -- Tracing sample rate (percentage, 1-100)
#    100 = trace all requests; 10 = trace 10% of requests
#    (OPTIONAL) / (DEFAULT: 10)
# tracing_sample_rate: 10

# -- Trace retention period
#    (OPTIONAL) / (DEFAULT: "72h")
# trace_retention: "72h"

# -- Trace storage size
#    (OPTIONAL) / (DEFAULT: "10Gi")
# trace_storage_size: "10Gi"

# -- Cluster name for trace metadata
#    (OPTIONAL) / (DEFAULT: "matherlynet")
# cluster_name: "matherlynet"

# -- Observability namespace for Tempo, Alloy, and other monitoring components
#    (OPTIONAL) / (DEFAULT: "monitoring")
# observability_namespace: "monitoring"

# -- Environment tag for traces and logs (production, staging, development)
#    (OPTIONAL) / (DEFAULT: "production")
# environment: "production"

# =============================================================================
# RUSTFS SHARED OBJECT STORAGE - S3-compatible storage for cluster services
# =============================================================================
# RustFS is a high-performance, Rust-based S3-compatible object storage system.
# When enabled, Loki automatically switches to SimpleScalable mode with S3.
# Performance: 2.3x faster than MinIO for small objects, Apache 2.0 license.
# REF: https://github.com/rustfs/rustfs
#
# IMPORTANT: RustFS does NOT support 'mc admin' commands for user/policy management.
# Access keys for services (Loki, etc.) must be created manually via the Console UI.
# REF: https://docs.rustfs.com/administration/iam/access-token.html
#
# SETUP WORKFLOW:
#   1. Enable rustfs_enabled and set rustfs_secret_key below
#   2. Run 'task configure' and 'task reconcile' to deploy RustFS
#   3. Wait for rustfs-bucket-setup job to complete (creates buckets)
#   4. Access RustFS Console at https://rustfs.<your-domain>
#   5. Login with rustfs_access_key / rustfs_secret_key (root credentials)
#   6. Create custom 'loki-storage' policy (Identity -> Policies -> Create Policy)
#      See: docs/research/rustfs-shared-storage-loki-simplescalable-jan-2026.md
#      Section: "Phase 2.5: Loki IAM Configuration" for policy JSON
#   7. Create 'monitoring' group with 'loki-storage' policy attached
#   8. Create 'loki' user in 'monitoring' group
#   9. Generate Access Key for 'loki' user (Service Accounts tab)
#   10. Copy access key and secret key (secret shown only once!)
#   11. Update loki_s3_access_key / loki_s3_secret_key below with generated values
#   12. Run 'task configure' and 'task reconcile' to apply Loki credentials
#
# NOTE: Tempo uses local filesystem storage by default, not RustFS/S3.
# NOTE: Custom 'loki-storage' policy scopes access to only loki-* buckets (least privilege).

# -- Enable RustFS shared storage
#    When enabled, Loki uses SimpleScalable mode with S3 backend
#    (OPTIONAL) / (DEFAULT: false)
# rustfs_enabled: false

# -- Number of RustFS nodes
#    Single node (1) for simple deployments, 4+ for distributed mode
#    Note: Helm chart creates data PVCs only for 1, 4, or 16 replicas
#    (OPTIONAL) / (DEFAULT: 1)
# rustfs_replicas: 1

# -- Data storage size per RustFS node
#    Used for volumeClaimTemplates via storageclass.dataStorageSize
#    (OPTIONAL) / (DEFAULT: "20Gi")
# rustfs_data_volume_size: "20Gi"

# -- Log storage size per RustFS node
#    Used for volumeClaimTemplates via storageclass.logStorageSize
#    (OPTIONAL) / (DEFAULT: "1Gi")
# rustfs_log_volume_size: "1Gi"

# -- StorageClass for RustFS PVCs
#    Falls back to global storage_class if not set
#    (OPTIONAL) / (DEFAULT: uses storage_class or "local-path")
# rustfs_storage_class: "proxmox-zfs"

# -- RustFS root admin username
#    (OPTIONAL) / (DEFAULT: "rustfsadmin")
# rustfs_access_key: "rustfsadmin"

# -- RustFS root admin password (SOPS-encrypted)
#    Generate with: openssl rand -base64 32
#    (REQUIRED when rustfs_enabled)
# rustfs_secret_key: ""

# -- Workload optimization profile
#    Options: GeneralPurpose, AiTraining, DataAnalytics, WebWorkload, IndustrialIoT, SecureStorage
#    (OPTIONAL) / (DEFAULT: "DataAnalytics")
# rustfs_buffer_profile: "DataAnalytics"

# -- Loki S3 access key (created manually via RustFS Console UI)
#    After deploying RustFS, create an access key in Console -> Identity -> Users
#    Copy the generated access key here
#    (REQUIRED when rustfs_enabled and loki_enabled)
# loki_s3_access_key: "PLACEHOLDER_CREATE_IN_RUSTFS_CONSOLE"

# -- Loki S3 secret key (created manually via RustFS Console UI, SOPS-encrypted)
#    Copy the generated secret key from RustFS Console here
#    (REQUIRED when rustfs_enabled and loki_enabled)
# loki_s3_secret_key: ""

# =============================================================================
# EXTERNAL SECRETS OPERATOR - Sync secrets from external providers
# =============================================================================
# External Secrets Operator syncs secrets from external secret stores.
# Supported providers: 1Password, Bitwarden, HashiCorp Vault
# REF: https://external-secrets.io/
# REF: docs/guides/k8s-at-home-remaining-implementation.md

# -- Enable External Secrets Operator
#    (OPTIONAL) / (DEFAULT: false)
# external_secrets_enabled: false

# -- External secrets provider: "1password", "bitwarden", or "vault"
#    (OPTIONAL) / (DEFAULT: "1password")
# external_secrets_provider: "1password"

# -- 1Password Connect host URL (required when provider is "1password")
#    (OPTIONAL) / (e.g. "http://onepassword-connect:8080")
# onepassword_connect_host: ""

# =============================================================================
# TALOS BACKUP - Automated etcd snapshots with S3 storage
# =============================================================================
# Talos Backup creates periodic etcd snapshots and uploads them to S3-compatible
# storage with Age encryption. Required for disaster recovery.
#
# OPTION A: Internal RustFS (when rustfs_enabled: true)
#   backup_s3_endpoint: "http://rustfs-svc.storage.svc.cluster.local:9000"
#   backup_s3_bucket: "etcd-backups"
#   - Create access key via RustFS Console (see docs/guides/talos-backup-rustfs-implementation.md)
#   - Benefits: No egress costs, low latency, unified storage
#   - Limitation: Backups lost if cluster is destroyed
#
# OPTION B: External Cloudflare R2 (true disaster recovery)
#   backup_s3_endpoint: "https://<account-id>.r2.cloudflarestorage.com"
#   backup_s3_bucket: "cluster-backups"
#   - Benefits: Survives cluster destruction, free egress
#   - Setup: Create R2 bucket + API token in Cloudflare dashboard
#
# REF: docs/guides/talos-backup-rustfs-implementation.md

# -- S3-compatible endpoint for backups
#    For RustFS internal: "http://rustfs-svc.storage.svc.cluster.local:9000"
#    For Cloudflare R2: "https://<account-id>.r2.cloudflarestorage.com"
#    (OPTIONAL) / (Enables talos_backup_enabled when set with backup_s3_bucket)
# backup_s3_endpoint: ""

# -- S3 bucket name for backups
#    For RustFS: "etcd-backups" (created by RustFS bucket setup job)
#    For R2: your bucket name (e.g. "cluster-backups")
#    (OPTIONAL)
# backup_s3_bucket: ""

# -- S3 access key ID
#    For RustFS: Create via Console UI with 'backup-storage' policy
#    For R2: Cloudflare API token with R2 write access
#    (OPTIONAL) / (Will be SOPS-encrypted after task configure)
# backup_s3_access_key: ""

# -- S3 secret access key
#    (OPTIONAL) / (Will be SOPS-encrypted after task configure)
# backup_s3_secret_key: ""

# -- S3 region (required by AWS SDK, any value works for S3-compatible storage)
#    (OPTIONAL) / (DEFAULT: "us-east-1")
# backup_s3_region: "us-east-1"

# -- Age public key for backup encryption (use same as cluster Age key)
#    Get from: cat age.key | grep "public key"
#    (OPTIONAL) / (e.g. "age1...")
# backup_age_public_key: ""

# =============================================================================
# CLOUDNATIVEPG OPERATOR - Production PostgreSQL for Kubernetes
# =============================================================================
# CloudNativePG provides production-grade PostgreSQL clusters with automated
# failover, backups, and monitoring. Deployed as a shared cluster resource.
# Single operator in cnpg-system namespace serves multiple database clusters.
# REF: https://cloudnative-pg.io/documentation/
# REF: docs/guides/cnpg-implementation.md
#
# SHARED RESOURCE PATTERN:
#   cnpg-system/           # Operator namespace (deployed by this config)
#   ├── cloudnative-pg     # Operator deployment
#   identity/              # App namespace (Keycloak example)
#   └── keycloak-postgres  # CNPG Cluster CR (app-specific)
#
# DEPENDENCY CHAIN: CloudNativePG → Keycloak → JWT/OIDC SecurityPolicy

# -- Enable CloudNativePG operator deployment
#    (OPTIONAL) / (DEFAULT: false)
# cnpg_enabled: false

# -- PostgreSQL image for new clusters
#    Options: 18.1-standard-trixie (default, with JIT), 18.1-minimal-trixie (smaller)
#    (OPTIONAL) / (DEFAULT: "ghcr.io/cloudnative-pg/postgresql:18.1-standard-trixie")
# cnpg_postgres_image: "ghcr.io/cloudnative-pg/postgresql:18.1-standard-trixie"

# -- Default storage class for CNPG clusters
#    Falls back to global storage_class if not set
#    (OPTIONAL) / (DEFAULT: uses storage_class or "local-path")
# cnpg_storage_class: ""

# -- Operator priority class
#    (OPTIONAL) / (DEFAULT: "system-cluster-critical")
# cnpg_priority_class: "system-cluster-critical"

# -- Deploy operator on control-plane nodes only
#    (OPTIONAL) / (DEFAULT: true for homelab single control-plane)
# cnpg_control_plane_only: true

# -- Enable CNPG backups to RustFS S3
#    Requires rustfs_enabled: true
#    Create access key via RustFS Console UI (see docs)
#    (OPTIONAL) / (DEFAULT: false)
# cnpg_backup_enabled: false

# -- S3 access key for CNPG backups (created via RustFS Console)
#    (REQUIRED when cnpg_backup_enabled: true)
# cnpg_s3_access_key: ""

# -- S3 secret key for CNPG backups (SOPS-encrypted)
#    (REQUIRED when cnpg_backup_enabled: true)
# cnpg_s3_secret_key: ""

# =============================================================================
# CLOUDNATIVEPG PGVECTOR EXTENSION - Vector similarity search for AI/ML
# =============================================================================
# pgvector provides native vector data types for similarity search, embedding
# storage, and AI-driven use cases. Mounted via ImageVolume (no custom images).
# REF: https://github.com/pgvector/pgvector
# REF: https://cloudnative-pg.io/docs/1.28/imagevolume_extensions/
#
# REQUIREMENTS:
#   - PostgreSQL 18+ (extension_control_path support)
#   - Kubernetes 1.35+ (ImageVolume beta enabled by default)
#   - CloudNativePG 1.27+ (extension mounting support)

# -- Enable pgvector extension for CNPG clusters
#    (OPTIONAL) / (DEFAULT: false)
# cnpg_pgvector_enabled: false

# -- pgvector extension image
#    (OPTIONAL) / (DEFAULT: "ghcr.io/cloudnative-pg/pgvector:0.8.1-18-trixie")
# cnpg_pgvector_image: "ghcr.io/cloudnative-pg/pgvector:0.8.1-18-trixie"

# -- pgvector version (must match image tag)
#    (OPTIONAL) / (DEFAULT: "0.8.1")
# cnpg_pgvector_version: "0.8.1"

# =============================================================================
# KEYCLOAK OIDC PROVIDER - Identity and Access Management
# =============================================================================
# Keycloak provides OIDC/OAuth2 authentication for JWT SecurityPolicy (API auth)
# and OIDC SecurityPolicy (web SSO). Deploys in the 'identity' namespace using
# the official Keycloak Operator (version-matched, CRD-based).
# REF: https://www.keycloak.org/operator/installation
# REF: docs/guides/keycloak-implementation.md
#
# DEPLOYMENT OPTIONS:
#   embedded: Uses in-cluster PostgreSQL StatefulSet (dev/testing)
#   cnpg: Uses CloudNativePG Cluster (production) - requires cnpg_enabled: true
#
# DEPENDENCY CHAIN: CloudNativePG (if cnpg mode) → Keycloak → JWT/OIDC SecurityPolicy

# -- Enable Keycloak deployment
#    (OPTIONAL) / (DEFAULT: false)
# keycloak_enabled: false

# -- Keycloak subdomain (creates auth.${cloudflare_domain})
#    (OPTIONAL) / (DEFAULT: "auth")
# keycloak_subdomain: "auth"

# -- Keycloak realm name (for application tokens)
#    (OPTIONAL) / (DEFAULT: "matherlynet")
# keycloak_realm: "matherlynet"

# -- Keycloak admin password (SOPS-encrypted)
#    Generate with: openssl rand -base64 24
#    Note: Admin username is always "admin" (operator default)
#    (REQUIRED when keycloak_enabled: true)
# keycloak_admin_password: ""

# -- Keycloak database mode: "embedded" or "cnpg"
#    embedded: Uses in-cluster PostgreSQL StatefulSet (dev/testing)
#    cnpg: Uses CloudNativePG Cluster (production, requires cnpg_enabled: true)
#    (OPTIONAL) / (DEFAULT: "embedded")
# keycloak_db_mode: "embedded"

# -- PostgreSQL credentials (SOPS-encrypted)
#    Used by both embedded PostgreSQL and CloudNativePG modes
#    (REQUIRED when keycloak_enabled: true)
# keycloak_db_user: "keycloak"
# keycloak_db_password: ""
# keycloak_db_name: "keycloak"

# -- Keycloak storage size (for PostgreSQL PVC)
#    (OPTIONAL) / (DEFAULT: "5Gi" for embedded, "10Gi" for cnpg)
# keycloak_storage_size: "5Gi"

# -- Keycloak replicas (1 for dev, 2+ for HA)
#    When >1, distributed Infinispan cache is enabled for session clustering
#    (OPTIONAL) / (DEFAULT: 1)
# keycloak_replicas: 1

# -- PostgreSQL instances (only for cnpg mode, 1 for dev, 3 for HA)
#    (OPTIONAL) / (DEFAULT: 1)
# keycloak_db_instances: 1

# -- Keycloak Operator version
#    (OPTIONAL) / (DEFAULT: "26.5.0")
# keycloak_operator_version: "26.5.0"

# -- Keycloak S3 access key for PostgreSQL backups (created via RustFS Console)
#    Create policy 'keycloak-storage' scoped to keycloak-backups bucket only
#    Create group 'identity' with keycloak-storage policy attached
#    Create user 'keycloak-backup' in identity group
#    (REQUIRED when keycloak_backup_enabled: true)
# keycloak_s3_access_key: ""

# -- Keycloak S3 secret key for PostgreSQL backups (SOPS-encrypted)
#    (REQUIRED when keycloak_backup_enabled: true)
# keycloak_s3_secret_key: ""

# -- Backup schedule for embedded mode pg_dump (cron format)
#    Only applies when keycloak_db_mode: "embedded"
#    CNPG mode uses continuous WAL archiving instead
#    (OPTIONAL) / (DEFAULT: "0 2 * * *" - daily at 2:00 AM UTC)
# keycloak_backup_schedule: "0 2 * * *"

# -- Backup retention days for embedded mode pg_dump
#    Only applies when keycloak_db_mode: "embedded"
#    CNPG mode uses retentionPolicy in barmanObjectStore
#    (OPTIONAL) / (DEFAULT: 7)
# keycloak_backup_retention_days: 7

# -- Enable Keycloak OpenTelemetry tracing
#    Requires tracing_enabled: true (Tempo deployed in monitoring namespace)
#    When enabled, Keycloak exports traces to Tempo via OTLP gRPC (port 4317)
#    (OPTIONAL) / (DEFAULT: false)
# keycloak_tracing_enabled: false

# -- Keycloak tracing sample rate (0.0 to 1.0)
#    1.0 = trace 100% of requests; 0.1 = trace 10% of requests
#    Production recommended: 0.1 (10%); Development: 1.0 (100%)
#    (OPTIONAL) / (DEFAULT: "0.1")
# keycloak_tracing_sample_rate: "0.1"

# =============================================================================
# OIDC/JWT CONFIGURATION - Optional for API authentication via SecurityPolicy
# =============================================================================
# JWT-based authentication for API endpoints, validating tokens against JWKS.
# When configured, creates a SecurityPolicy targeting HTTPRoutes with
# label "security: jwt-protected".
# REF: https://gateway.envoyproxy.io/latest/concepts/gateway_api_extensions/security-policy/
# REF: docs/guides/envoy-gateway-observability-security.md

# -- OIDC provider name (used in SecurityPolicy)
#    (OPTIONAL) / (DEFAULT: "keycloak")
# oidc_provider_name: "keycloak"

# -- OIDC issuer URL (JWT token issuer - must match "iss" claim in tokens)
#    (OPTIONAL) / (e.g. "https://auth.example.com/realms/myrealm")
# oidc_issuer_url: ""

# -- OIDC JWKS URI for JWT validation (public keys endpoint)
#    (OPTIONAL) / (e.g. "https://auth.example.com/realms/myrealm/protocol/openid-connect/certs")
# oidc_jwks_uri: ""

# -- Additional claims to extract from JWT and pass as headers
#    (OPTIONAL) / Headers must start with "X-"
#    Example:
#    oidc_additional_claims:
#      - name: "preferred_username"
#        header: "X-Username"
#      - name: "realm_access.roles"
#        header: "X-User-Roles"
# oidc_additional_claims: []

# =============================================================================
# VOLSYNC PVC BACKUP - Automated PVC backups with restic to S3
# =============================================================================
# VolSync provides restic-based PVC backups to S3-compatible storage.
# Enables point-in-time recovery for stateful applications.
# REF: https://volsync.readthedocs.io/en/stable/
# REF: docs/guides/k8s-at-home-remaining-implementation.md

# -- Enable VolSync PVC backup
#    (OPTIONAL) / (DEFAULT: false)
# volsync_enabled: false

# -- S3-compatible endpoint for backups (Cloudflare R2 recommended)
#    (OPTIONAL) / (e.g. "https://<account-id>.r2.cloudflarestorage.com")
# volsync_s3_endpoint: ""

# -- S3 bucket name for VolSync backups
#    (OPTIONAL) / (e.g. "cluster-pvc-backups")
# volsync_s3_bucket: ""

# -- Restic repository password for encryption
#    (OPTIONAL) / (Will be SOPS-encrypted after task configure)
# volsync_restic_password: ""

# -- Backup schedule (cron format)
#    (OPTIONAL) / (DEFAULT: "0 */6 * * *" - every 6 hours)
# volsync_schedule: "0 */6 * * *"

# -- Copy method for creating backups: "Clone" or "Snapshot"
#    Clone: Works with any CSI driver supporting volume cloning (Proxmox CSI)
#    Snapshot: Requires CSI driver with VolumeSnapshot support
#    (OPTIONAL) / (DEFAULT: "Clone")
# volsync_copy_method: "Clone"

# -- Daily backup retention count
#    (OPTIONAL) / (DEFAULT: 7)
# volsync_retain_daily: 7

# -- Weekly backup retention count
#    (OPTIONAL) / (DEFAULT: 4)
# volsync_retain_weekly: 4

# -- Monthly backup retention count
#    (OPTIONAL) / (DEFAULT: 3)
# volsync_retain_monthly: 3

# =============================================================================
# GLOBAL VM DEFAULTS (OpenTofu/Proxmox) - Optional for VM deployments
# =============================================================================
# -- Global VM resource defaults (fallback for all nodes)
#    Can be overridden by role-specific defaults or per-node in nodes.yaml
#    Fallback chain: per-node -> role-defaults -> global-defaults

# proxmox_vm_defaults:
#   cores: 4           # CPU cores
#   sockets: 1         # CPU sockets
#   memory: 8192       # Memory in MB
#   disk_size: 128     # Disk size in GB
#
# -- Controller node VM defaults (optimized for etcd and control plane)
#    Controllers run etcd, API server, scheduler, and controller-manager
#    When allowSchedulingOnControlPlanes: false (default), no workloads run here
#    Fallback chain: per-node -> controller-defaults -> global-defaults
# proxmox_vm_controller_defaults:
#   cores: 4           # CPU cores (etcd is single-threaded, 4 is plenty)
#   sockets: 1         # CPU sockets
#   memory: 8192       # Memory in MB (8GB sufficient for control plane)
#   disk_size: 64      # Disk size in GB (etcd only, no workloads)
#
# -- Worker node VM defaults (optimized for running workloads)
#    Workers run application pods, require more resources
#    Fallback chain: per-node -> worker-defaults -> global-defaults
# proxmox_vm_worker_defaults:
#   cores: 8           # CPU cores (more for workload scheduling)
#   sockets: 1         # CPU sockets
#   memory: 16384      # Memory in MB (16GB for application pods)
#   disk_size: 256     # Disk size in GB (container images + workloads)
#
# -- Advanced VM settings (Talos-optimized defaults)
# proxmox_vm_advanced:
#   bios: "ovmf"              # UEFI boot (required for Talos)
#   machine: "q35"            # Modern chipset
#   cpu_type: "host"          # CPU type (host = passthrough)
#   scsi_hw: "virtio-scsi-pci"# SCSI controller
#   balloon: 0                # Memory ballooning disabled (K8s)
#   numa: true                # NUMA enabled
#   qemu_agent: true          # QEMU guest agent
#   net_queues: 4             # Multi-queue networking
#   disk_discard: true        # Enable TRIM/discard
#   disk_ssd: true            # SSD emulation
#   tags: ["kubernetes", "linux", "talos"]  # VM tags
#   # Network configuration
#   network_bridge: "vmbr0"   # Proxmox bridge interface for VM networking
#   # Guest OS configuration
#   ostype: "l26"             # Linux 2.6/3.x/4.x/5.x/6.x kernel
#   # Storage flags (Talos-optimized: immutable OS, K8s handles HA)
#   disk_backup: false        # Exclude from Proxmox backup jobs
#   disk_replicate: false     # Disable Proxmox replication
